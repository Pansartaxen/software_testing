{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Unit tests and coverage\n",
    "\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Coverage](#2.-Coverage)\n",
    "    - [2.1 Statement coverage](#2.1-Statement-coverage)\n",
    "    - [2.2 Branch coverage](#2.2-Branch-coverage)\n",
    "    - [2.3 Dataflow coverage](#2.3-Dataflow-coverage)\n",
    "- [3. More unit tests](#3.-More-unit-tests)\n",
    "- [4. Mocking](#4.-Mocking)\n",
    "- [5. Coverage revisited](#5.-Coverage-revisited)\n",
    "- [BONUS: `doctest`](#BONUS:-doctest)\n",
    "- [6. Submit to Canvas](#6.-Submit-to-Canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "For a new self-driving car, we need an implementation of a high-precision pi: ChatGPT v4 suggests the following implementation for computing pi in Python, including a unit test. The code is packed in the two files `estimate_pi.py` and `test_estimate_pi.py`. \n",
    "\n",
    "Run the existing test using your shell (every cell starting with an `!` will be executed in your OS's shell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /Users/mariusstokkedal/Desktop/PA1465_Test_av_mjukvara/Assignment_1\n",
      "plugins: anyio-3.5.0, mock-3.7.0, cov-3.0.0\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_estimate_pi.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.40s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest test_estimate_pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is problematic with that test ChatGPT created for us and would you address this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function for the function `estimate_pi` is only tested with one valid value. If a value less than 0 is sent as argument it will return **π = 0** which is undeniably incorrect; and if 0 is inserted the program will crash when deviding by 0 (`return 4 * count / n`). Since this function is meant to be located in a self driving car errors could have devastating consequences. If **π** is estimated to 0, calculating for example speed based on wheel rotation speed will always be **0**.  Further more, *delta* should be smaller than 0.01 because a **high precission** is requested. Lastly, the main function in `estimate_py.py`is not tested, but I assume this is not the objective of the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Statement coverage\n",
    "Compute the statement coverage of the program using [`coverage.py`](https://coverage.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.014s\n",
      "\n",
      "OK\n",
      "Name                     Stmts   Miss  Cover   Missing\n",
      "------------------------------------------------------\n",
      "OG_estimate_pi.py           26     12    54%   18-19, 23-36\n",
      "OG_test_estimate_pi.py       9      0   100%\n",
      "------------------------------------------------------\n",
      "TOTAL                       35     12    66%\n"
     ]
    }
   ],
   "source": [
    "!coverage run -m OG_test_estimate_pi\n",
    "!coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report tells us that 54% of the statements are executed in`estimate_pi.py` and 100% in `test_estimate_pi.py`. However, the lines marked as \"Missing\" are the main code for both of the files. This code is only run if the code is run directly by a command and not being imported to another file. Since the files are run from covareage and not directly the main code will not be running and is therefore marked as missing. The lines 18-19 in `estimate_pi.py`are located in the method `write`from the class `PiFileWriter`, this is not called from the test file and therefore not run.\n",
    "\n",
    "With this in mind the coverage of the method `estimate_pi`is 100% but the `PiFileWriter`-class is not called and the code isn't run as the main file resulting in the main code not being run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Branch coverage\n",
    "Now compute the statement coverage of the program using [`coverage.py`](https://coverage.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 2.191s\n",
      "\n",
      "OK\n",
      "Name                     Stmts   Miss Branch BrPart  Cover   Missing\n",
      "--------------------------------------------------------------------\n",
      "OG_estimate_pi.py           26     12     10      1    53%   18-19, 23-36\n",
      "OG_test_estimate_pi.py       9      0      2      1    91%   11->exit\n",
      "--------------------------------------------------------------------\n",
      "TOTAL                       35     12     12      2    62%\n"
     ]
    }
   ],
   "source": [
    "!coverage run --branch OG_test_estimate_pi.py\n",
    "!coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interprete the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing lines are the same as in 2.1 and the branch coverage can therefor be interpreted as 100% since we only want to test the function `estimate_pi`. The column *BrPart* is short for *Branch Partial* and refers to `if __name__ == '__main__':`, this is partial since it is always true if the code is main and always false if it is imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataflow coverage\n",
    "\n",
    "Draw the flow graph for the function `estimate_pi` defined in `estimate_pi.py`. Annotate the graph with definition and use information. Note: Please submit a separate image file or PDF with the name `dataflow_coverage.<file_extension>` for this task.\n",
    "\n",
    "Identify and describe the minimum number of test cases to achieve: all-defs coverage, and all-uses coverage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All-defs\n",
    "To cover all defs of all variables in `estimate_pi` the shortest possible path is the nodes: **1, 2, 3, 4, 5, 2, 6**. Node *4* is a if-statement depending on the result from node *3* which is a random value. To ensure node *5* is executed **one** test case is enoug assuming the argument *N* is large enough, e.g. **1000**, to ensure that node *4's* condition is true at least once.\n",
    "\n",
    "#### All-uses\n",
    "In the case of *all-use* the result is almost the same as in *all-defs* but it requires all possible outcomes of a if-statement or conditional for-loop to be covered. In this cas we have both. The shortest possible path for *all-use* is therefore **1, 2, 3, 4, 2, 3, 4, 5, 2, 6** alternativly **1, 2, 3, 4, 5, 2, 3, 4, 2, 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. More unit tests\n",
    "\n",
    "Add two more unit tests with the principles you learned in the lecture. Describe what principle you have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_estimate_pi_illigal_argument(self):\n",
    "    with self.assertRaises(ValueError):\n",
    "        estimate_pi(-1)\n",
    "            \n",
    "def test_estimate_pi_wrong_data_type():\n",
    "    with self.assertRaises(TypeError):\n",
    "            estimate_pi('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.I.R.S.T\n",
    "**F.I.R.S.T** is short for Fast, Isolated, Repeateble, Self-validating and Thorough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F – Fast\n",
    "Both tests will have finite running time and is therefore considered as fast in this case.\n",
    "\n",
    "### I – Isolated\n",
    "Both tests are isolated since they only need them selves and the function `estimate_pi` to be executed.\n",
    "\n",
    "### R – Repeateble\n",
    "Both test are independent from the machine they are executed on, considering *operating system, software and hardware*. A test of `PiFileWriter` would be hard to make repeateble since the disk may be accesed differently on other operating ssystems. Stationary values are also used, i.e. they are not for example generated by a random value\n",
    "\n",
    "### S – Self-validating\n",
    "Both tests are self-validating since we know what the desired output is and therefore they also achives this criteria.\n",
    "\n",
    "### T – Thorough\n",
    "As discussed earlier, the original test lacks a test for invalid values. With these two tests we test with an input that is too small and another which is of an invalid type (string instead of int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Mocking\n",
    "\n",
    "We want to store the resulting number persistently on our file system. We use the following class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiFileWriter:\n",
    "    @staticmethod\n",
    "    def write(content, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a test double for `PiFileWriter` and add your implementation to `test_estimate_pi.py`. Discuss what type of test double you have implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I replaced the class `PiFileWriter`with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockPiFileWriter:\n",
    "    def __init__(self):\n",
    "        self._content = None\n",
    "        self._file_path = None\n",
    "\n",
    "    def write(self, content, file_path):\n",
    "        self._content = content\n",
    "        self._file_path = file_path\n",
    "\n",
    "    def content(self):\n",
    "        return self._content\n",
    "\n",
    "    def file_path(self):\n",
    "        return self._file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then added this method to `test_estimate_pi.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPiFileWriter(self):\n",
    "    mock_file_writer = MockPiFileWriter()\n",
    "    estimate = estimate_pi(1000000)\n",
    "    path = '/test.txt'\n",
    "    mock_file_writer.write(estimate, path)\n",
    "    self.assertEqual(mock_file_writer.content(), estimate)\n",
    "    self.assertEqual(mock_file_writer.file_path(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mock saves the *content* and the *file path* as variables in the instance of the class instead of acctually writing the content to the given adress on the disk. This makes it possible to run the same test code on multiple different operative systems without the need of modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name three other types of unit tests you would want to mock and explain why.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Since `estimate_pi` relies on random values for the variables *x* and *y* it can be hard to determine which values they will take. Therefore `random.uniform(-1, 1)` should be mocked with a function that behaves like `random`, but also generates predicteble numbers. This will result in `estimate_pi` being easier to test since its result will always be reproduceble and the test will not have to relie on any external code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "In a more general scope database access should be mocked. Relying on a databaase during testing can both increase the run time of the test and affect the data it self by potentially producing inconsistencies or corrupt data. If the database instead is mocked, we can simulate the data and test the code's behavior and performance witout relying on a database and its data behaving as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "Lastly, time-related functions is important to mock during testing since their output is affected by, as the name suggests, the current time for the test. By mocking the time-related functions, we can simulate different time scenarios and test the behavior of our code. This is particularly important for testing time-sensitive logic, such as expiration dates or time-based calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Coverage revisited\n",
    "\n",
    "Rerun statement and branch coverage and discuss the differences and changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 4.208s\n",
      "\n",
      "OK\n",
      "Name                  Stmts   Miss  Cover   Missing\n",
      "---------------------------------------------------\n",
      "estimate_pi.py           32     10    69%   32-45\n",
      "test_estimate_pi.py      16      0   100%\n",
      "---------------------------------------------------\n",
      "TOTAL                    48     10    79%\n"
     ]
    }
   ],
   "source": [
    "!coverage run -m test_estimate_pi\n",
    "!coverage report -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 4.410s\n",
      "\n",
      "OK\n",
      "Name                  Stmts   Miss Branch BrPart  Cover   Missing\n",
      "-----------------------------------------------------------------\n",
      "estimate_pi.py           32     10     10      1    64%   32-45\n",
      "test_estimate_pi.py      16      0      2      1    94%   19->exit\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                    48     10     12      2    73%\n"
     ]
    }
   ],
   "source": [
    "!coverage run --branch test_estimate_pi.py\n",
    "!coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage for *cover* went up for `estimate_pi.py`in both of the testa. This is both because there is one less line marked as \"Missing\" and the ratio of code being tested versus not tested went up resulting in a higher coverage. But since the added code is a mock no extra lines of code are acctually tested in the new version. The run time on the other hand has now almost dubbeled, goning from *~2 seconds* to *~4 seconds*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# BONUS: `doctest`\n",
    "\n",
    "If you are curious or want to stand out, check out [`doctest`](https://en.wikipedia.org/wiki/Doctest). This task is optional. \n",
    "\n",
    "Add two `doctest` test cases and run the `doctest` tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you like `doctest`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submit to Canvas\n",
    "\n",
    "Almost done, but the most tricky part is missing: submitting. :)\n",
    "\n",
    "Before submitting, make sure\n",
    "- you completed all non-optional tasks in this assignment (i.e., all empty cells are filled with meaningful content)\n",
    "- you don't use external libraries except `coverage.py`\n",
    "- the notebook runs straight through\n",
    "- your test code works\n",
    "- your code is readable and follows the Python coding conventions\n",
    "\n",
    "All set? Great. Just two steps away from happiness. \n",
    "\n",
    "1. Go through the list above and check again\n",
    "2. Submit *three* files to canvas:\n",
    "    - `assignment.ipynb`\n",
    "    - `test_estimate_pi.py`\n",
    "    - `dataflow_coverage.<file_extension>`\n",
    "3. Take a deep breath and carpe diem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
